---
title: "DS2_HW1"
author: "Yixiao Sun"
date: "2024-02-14"
output: html_document
---

```{r}
library(ISLR)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
library(ggrepel)
```

```{r}
train_data <- read.csv("~/Desktop/P8106 Data Science 2/ds2_hw1/housing_training.csv")
test_data <- read.csv("~/Desktop/P8106 Data Science 2/ds2_hw1/housing_test.csv")
x<-model.matrix(Sale_Price ~., train_data)[,-26]
y<-train_data[,"Sale_Price"]
```

# Lasso Model 
```{r}
set.seed(2)
ctrl1 <- trainControl(method = "cv", number = 10)
ctrl2 <- trainControl(method = 'cv', number = 10, selectionFunction = "oneSE")

lasso.fit <- train(Sale_Price ~ .,
                   data = train_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(6, 0, length = 100))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune # Best Tune

lasso.pred <- predict(lasso.fit, newdata = test_data)

lasso_mse <- mean((lasso.pred - test_data[,"Sale_Price"])^2) # LASSO MSE
lasso_mse
```
### The selected tuning parameter for the Lasso Model is 48.36555, the test error is 441688534

```{r}
set.seed(2)
lasso.fit_2 <- train(Sale_Price ~ .,
                   data = train_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,lambda = exp(seq(8, 0, length = 100))),
                   trControl = ctrl2)
plot(lasso.fit_2, xTrans = log)
lasso.fit_2$bestTune

coef(lasso.fit_2$finalModel, lasso.fit$bestTune$lambda)
sum(coef(lasso.fit_2$finalModel, s=lasso.fit_2$bestTune$lambda) != 0) - 1
```
### WHen the 1SE rule applied, 35 predictors are inclueded in the model.



# Elastic Net
```{r}
set.seed(2)
enet.fit <- train(Sale_Price ~ .,
                  data = train_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(7, 0, length = 100))),
                  trControl = ctrl1)
enet.fit$bestTune # ELastic Net Best Tune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

enet.predic <- predict(enet.fit, newdata = test_data)
enet.error <- mean((enet.predic - test_data[,"Sale_Price"])^2)
enet.error
enet.fit_2 <- train(Sale_Price ~ .,
                  data = train_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(7, 0, length = 100))),
                  trControl = ctrl2)
enet.fit_2$bestTune




```
### The selected tuning parameter for the elastic net model is 286.1642, the test error is 439998442 The 1SE rule can't be applied to this model, since the best tune given by the 1SE model print out an alpha with 0.





# Least Square Model
```{r}
set.seed(2)
x2 <- model.matrix(Sale_Price ~ .,test_data)[, -26]
y2 <- test_data$Sale_Price
pls.fit <- train(Sale_Price ~.,
                 data = train_data,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:20),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))
predy2.pls2 <- predict(pls.fit, newdata = test_data)

mean((y2 - predy2.pls2)^2) # least square model MSE

ggplot(pls.fit, highlight = TRUE)

```
### The test error for the least square model is 446775692. 16 Components are included in this model based on the plot.



# Model Comparison
```{r}
set.seed(2)
lm.fit <- train(Sale_Price ~ .,data = train_data, method = "lm", trControl = ctrl1)
resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, pls = pls.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```
### Based on the graph given, the Elastic net model is the best model for predicting the response since it has the lowest RMSE.

# Tidymodels
```{r}
set.seed(2)
cv_folds <- vfold_cv(train_data, v = 10)

enet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

enet_grid_set <- parameters(penalty(range = c(0, 7), 
                                    trans = log_trans()),
                            mixture(range = c(0, 1)))

enet_grid <- grid_regular(enet_grid_set, levels = c(100, 21))

enet_workflow <- workflow() %>%
  add_model(enet_spec) %>%
  add_formula(Sale_Price ~ .)

enet_tune <- tune_grid(
  enet_workflow,
  resamples = cv_folds,
  grid = enet_grid
)

autoplot(enet_tune, metric = "rmse") + theme(legend.position = "top") + labs(color = "Mixing Percentage\n(Alpha Values)")

enet_best <- select_best(enet_tune, metric = "rmse")

enet_best
```
### The tuning parameter for the tidymodel is 668.5094. The potential reasons for the differences when chosing parameters can be due to the different ways they grid search which produce the wanted value.
